import pandas as pd
import seaborn as sns
import numpy as np
import sqlite3
from scikitplot.helpers import cumulative_gain_curve 
from sklearn.metrics import auc
from sklearn.metrics import roc_auc_score
import plotly.express as px

recency frequency monetary

r (как давно что-то покупал? Чем более недавно, тем лучше) 1-4
f (как часто покупал ранее? Чем чаще, тем лучше) 1-4
m (как много денег потратил на нас? Чем больше, тем лучше) 1-4

df = pd.read_csv('data.csv', encoding='cp1252')

df.head()

Сразу переведу дату в тип дата здесь, в питоне. И в sqlite уже все будет ок.

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'],format='%m/%d/%Y %H:%M')

Теперь создадим 1) саму базу и подключкние к ней 2) курсор -- понадобится чтобы создавить таблички 3) функцию обертку для того чтобы быстро писать селекты

df.info()

conn = sqlite3.connect('db')
cur = conn.cursor()

def select(sql):
    return pd.read_sql(sql,conn)

Теперь зальем таблу и проверим, что все ок

df.to_sql('ecommerce',conn,index=False,if_exists='replace')

sql = '''select count(*) from ecommerce t'''

Смотрю сколько транзакций всего и сколько ежемесячно

select(sql)

len(df)

sql = '''select 
                date(t.InvoiceDate,'start of month') as date,
                count(*) as count
                from ecommerce t
                group by date
                order by date
                '''
select(sql)

Декабрь не до конца, его брать не буду

sql= '''select max(t.InvoiceDate) from ecommerce t'''
select(sql)

Избавлюсь от Null, посмотрю дату регистрации и сразу посчитаю revenue

sql = '''select 
                    t.CustomerID,
                    min(t.InvoiceDate) as reg_date,
                    sum(t.Quantity * t.UnitPrice) as revenue
                   
                    from ecommerce t
                    where date(t.InvoiceDate,'start of month') <= '2011-10-01'
                    and t.CustomerID is not null
                    group by t.CustomerID
                            '''
t = select(sql)
t

Смотрю кто есть с отрицательным revenue, сначала хотел почистить их, но потом подумал, что это же тоже покупатели, они либо делали возврат, либо была какая-то акция, то есть нужно их учитывать тоже.


t[t['revenue']<0]

почищу тех кто имеет меньше 10000 revenue, чтобы убрать сильные выбросы, так модель будет честнее, их всего ~1%

t[t['revenue']<10000]['revenue'].hist()

100 - len(t[t['revenue']<10000]['revenue']) / len(t)

Беру данные до ноября и буду лефт джойнить с главной таблицей,чтобы найти дату последней транзакции

sql = '''
select 
    t.CustomerID, 
    sum(t.Quantity * t.UnitPrice) as target
from ecommerce t
where date(t.InvoiceDate,'start of month') == '2011-11-01'
group by CustomerID
'''
select(sql)

Добавляю Frequency, monetory и создаю таблицу, с которой буду работать

sql = '''
drop table if exists customer;
create table customer as 
    select t.*,
    date('2011-11-01') as month,
    julianday(date('2011-11-01')) - julianday(t.last_purchase) as recency
    
    from
    (   select 
        t.CustomerID,
        min(t.InvoiceDate) as reg_date,
        max(t.InvoiceDate) as last_purchase,
        sum(t.Quantity * t.UnitPrice) as monetory,
        count(distinct t.InvoiceNo) as frequency
        
        from ecommerce t
        where date(t.InvoiceDate,'start of month') <= '2011-10-01'
        and t.CustomerID is not null
        group by t.CustomerID
    ) t
        where t.monetory < 10000
        
'''
cur.executescript(sql)

sql = '''select t.*, n.target from customer t
left join (
select 
    t.CustomerID, 
    sum(t.Quantity * t.UnitPrice) as target
from ecommerce t
where date(t.InvoiceDate,'start of month') == '2011-11-01'
group by CustomerID
) n on t.CustomerID = n.CustomerID
'''
t = select(sql)
t

sql= '''select * from customer t '''
select(sql)

Начинаю сегментацию, вычитаю из пятерки и прибаляю 1, чтобы сегментировало удобнее, где 1-не частый покупатель, 4 частый

t['r'] = 5 - (pd.qcut(t['recency'],4,labels=False)+1)

t

проверяю, как поделил qcut моих пользователей

t.groupby('r')['recency'].agg(['mean','count'])

Смотрю на данные по frequency

t['frequency'].value_counts()

Сегментация для Frequency

t['f'] = pd.qcut(t['frequency'],4,labels=False,duplicates='drop') +1

t.groupby('f')['frequency'].agg(['mean','count']) 

t.groupby('f')['frequency'].agg(set) 

Мне не понравилась такая сегментация, потому что сильная скошенность данных, сделаю свою функцию для сегментации

def get_f(x):
    if x == 1:
        return 1
    if x in [2,3]:
        return 2
    if x in [4,5,6]:
        return 3
    return 4

t['f'] = t['frequency'].apply(get_f)

t.groupby('f')['frequency'].agg(['mean','count']) 

такая сегентация более честная

И последняя сегментация для monetory

t['m'] = pd.qcut(t['monetory'],4,labels=False) +1

t.groupby('m')['monetory'].agg(['mean','count']) 

Посчитаю RFM_Score

t['rfm_score'] = (t['r'] + t['f'] + t['m'])/3


t

t.sort_values('rfm_score', ascending=False).head(1000)

Сделаю еще более наглядную сегментацию для Маркетологов

def rfm_level(df):
    if df['segment_score'] >= 9:
        return 'Can\'t Loose Them'
    elif ((df['segment_score'] >= 8) and (df['segment_score'] < 9)):
        return 'Champions'
    elif ((df['segment_score'] >= 7) and (df['segment_score'] < 8)):
        return 'Loyal'
    elif ((df['segment_score'] >= 6) and (df['segment_score'] < 7)):
        return 'Potential'
    elif ((df['segment_score'] >= 5) and (df['segment_score'] < 6)):
        return 'Promising'
    elif ((df['segment_score'] >= 4) and (df['segment_score'] < 5)):
        return 'Needs Attention'
    else:
        return 'Require Activation'

t['segment_score'] = (t['r'] + t['f'] + t['m'])

t['segment'] = t.apply(rfm_level,axis=1)
t

Количество клиентов по сегментам

t.groupby('segment')['monetory'].agg(['mean','count'])

t

Посмотрю как распределились сумма Monetory и target 

t.pivot_table(index='f',columns='r', values='monetory',aggfunc='sum')

sns.heatmap(t.pivot_table(index='f',columns='r', values='monetory',aggfunc='sum'))

sns.heatmap(t.pivot_table(index='f',columns='r', values='target',aggfunc='sum'))

t['has_purchase'] = (t['target'] > 0) *1

t['has_purchase'].mean()

sns.heatmap(t.pivot_table(index='f',columns='r', values='has_purchase',aggfunc='mean'))

t.pivot_table(index='f',columns='r',values='has_purchase')

t['has_purchase'].sum()

len(t)

t.sample(frac=0.2)['has_purchase'].sum()/t['has_purchase'].sum()

t.sort_values('rfm_score',ascending=False,inplace=True)

t

t.head(round(len(t)*0.2))['has_purchase'].sum()/t['has_purchase'].sum()

#в 1,875 раза лучше чем рандом uplift
t.head(round(len(t)*0.2))['has_purchase'].sum()/t['has_purchase'].sum() / 0.2

per, gain = cumulative_gain_curve(t['has_purchase'], t['rfm_score'])

lc = pd.DataFrame({'per':per,'gain':gain})

lc['random']= lc['per']

lc.plot(x='per',y=['gain','random'],figsize=(6,6),grid=True)

t['has_purchase'].mean()

_,ideal = cumulative_gain_curve(t['has_purchase'], t['has_purchase'])

lc['ideal']= ideal

lc.plot(x='per',y=['gain','random','ideal'],figsize=(6,6),grid=True)

#площадь между random и синей кривой
model_auc = auc(lc['per'],lc['gain'])- 0.5
model_auc

#площадь идеальной модели между зеленой прямой и random
ideal_auc = auc(lc['per'],lc['ideal'])- 0.5
ideal_auc

#на сколько приблизился к идеалу
gini = model_auc / ideal_auc
gini

rocauc = roc_auc_score(t['has_purchase'], t['rfm_score'])
rocauc

2 * rocauc -1 

gini

Не совпадает потому что скоры не уникальные. Можно добавить шум, и увидеть, что все работает.

t['rfm_score'].value_counts()

np.random.uniform(size=len(t)) / 1000000

t['rfm_score_noise'] = t['rfm_score']+(np.random.uniform(size=len(t)) / 1000000)

per, gain = cumulative_gain_curve(t['has_purchase'],t['rfm_score_noise'])
lc = pd.DataFrame({'per':per,'gain':gain})
lc['random'] = lc['per']
_, ideal = cumulative_gain_curve(t['has_purchase'],t['has_purchase'])
lc['ideal'] = ideal

model_auc = auc(lc['per'],lc['gain']) - 0.5
ideal_auc = auc(lc['per'],lc['ideal']) - 0.5
gini = model_auc / ideal_auc

gini

rocuac = roc_auc_score(t['has_purchase'],t['rfm_score_noise'])

2 * rocuac - 1

Теперь все совпадает

Добавим красоты)

t.columns

t['r_pct_rn'] = t['recency'].rank(pct=True, ascending=False).round(2)
t['f_pct_rn'] = t['frequency'].rank(pct=True).round(2)
t['m_pct_rn'] = t['monetory'].rank(pct=True).round(2)

matrix = (t[t['monetory']>0].groupby(['r_pct_rn', 'f_pct_rn', 'm_pct_rn'])[['monetory']].sum()
            .reset_index())

fig = px.scatter_3d(matrix, x='r_pct_rn', y='f_pct_rn', z='m_pct_rn',
              color='monetory', opacity=0.15)
fig.show()

matrix = (t[t['monetory']>0].groupby(['r', 'f', 'm'])[['monetory']].sum()
            .reset_index())

fig = px.scatter_3d(matrix, x='r', y='f', z='m',
              color='monetory')
fig.show()





